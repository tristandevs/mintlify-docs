The ShuttleAI API provides the ability to stream responses back to a client in order to allow partial results for certain requests. To achieve this, we follow the Server-sent events standard.

Our official [Python Library](https://pypi.org/project/shuttleai) handles Server-sent events for you. In Python, a streaming request looks like:

<CodeGroup>
```python Python
from shuttleai import *
import asyncio

async def main():
    async with ShuttleAsyncClient($SHUTTLEAI_API_KEY, timeout=60) as shuttle:
         response = await shuttle.chat_completion(
             model="gpt-4",
             messages="write me a short story about bees",
             stream=True,
             plain=True
         )
         async for chunk in response:
             print(chunk.choices[0].delta.content)


if __name__ == "__main__":
    asyncio.run(main())
```

```javascript JavaScript
const axios = require('axios');

async function main() {
    try {
        const response = await axios.post('https://api.shuttle.ai/chat_completion', {
            model: 'gpt-4',
            messages: [{role: 'user', content: 'write me a short story about bees'}],
            stream: true
        }, {
            'Authorization': 'Bearer $SHUTTLEAI_API_KEY',
            'Content-Type': 'application/json
            }
        );

        if (response.data) {
            response.data.forEach(chunk => {
                try {
                    console.log(chunk.choices[0].delta.content);
                } catch (error) {
                    // Handle error
                }
            });
        }
    } catch (error) {
        // Handle error
    }
}

main();
```
</CodeGroup>

Continue to [Get Models](/api-reference/endpoint/models) to get started!

<Tip>All additional optional kwargs will be listed in `Endpoints` category, be sure to check it out!</Tip>